{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA8HXX0w_L3S",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "PhqG8g0_hRgZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPeSl1dICo98"
   },
   "source": [
    "## PQMass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "f0C5R6EQCsN_"
   },
   "outputs": [],
   "source": [
    "# Function to compute the chi2 statistic for the PQM test\n",
    "def get_pqm_chi2(x_samples, y_samples, num_refs=100):\n",
    "    \"\"\"\n",
    "    Compute the chi2 statistic for the PQM test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_samples : np.ndarray\n",
    "        Samples from the posterior.\n",
    "    y_samples : np.ndarray\n",
    "        True samples\n",
    "    num_refs : int\n",
    "        Number of reference samples to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Chi2_pqm statistic.\n",
    "    \"\"\"\n",
    "    # Randomly select reference samples from y_samples\n",
    "    refs = np.random.choice(len(y_samples), num_refs, replace=False)\n",
    "    refs = y_samples[refs]\n",
    "\n",
    "    # Initialize counts for x_samples and y_samples\n",
    "    counts_x = np.zeros(num_refs, dtype=\"int\")\n",
    "    counts_y = np.zeros(num_refs, dtype=\"int\")\n",
    "\n",
    "    # Count occurrences for x_samples\n",
    "    for x in x_samples:\n",
    "        d = np.linalg.norm(x.reshape(1, -1) - refs, axis=-1)\n",
    "        idx = np.argmin(d)\n",
    "        counts_x[idx] += 1\n",
    "\n",
    "    # Count occurrences for y_samples\n",
    "    for y in y_samples:\n",
    "        d = np.linalg.norm(y.reshape(1, -1) - refs, axis=-1)\n",
    "        idx = np.argmin(d)\n",
    "        counts_y[idx] += 1\n",
    "\n",
    "    # Compute chi2 statistic using contingency table\n",
    "    chi2_stat, _, _, _ = chi2_contingency(np.array([counts_x, counts_y]))\n",
    "    return chi2_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EkKBV6__Ov6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classes & Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zASdl0ahht0Q"
   },
   "outputs": [],
   "source": [
    "# Actor network class to represent the policy\n",
    "# Gives a probability distribution for the actions\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        # Output layer for mean and variance\n",
    "        self.mean_layer = nn.Linear(256, state_size)\n",
    "        self.variance_layer = nn.Linear(256, state_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        mean = self.mean_layer(output)\n",
    "        variance = F.softplus(self.variance_layer(output))  # Using softplus to ensure variance is positive\n",
    "        return mean, variance\n",
    "\n",
    "# Critic network class to represent the value function\n",
    "# Estimates the expected reward\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "ktes3ppD8KbT"
   },
   "outputs": [],
   "source": [
    "# Custom environment class to simulate the problem setting without gym\n",
    "class CustomEnv:\n",
    "    def __init__(self, function, sample_size, num_refs=100, linspace=None):\n",
    "        \"\"\"\n",
    "        Initialize the custom environment.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        function : callable\n",
    "            The function to sample from.\n",
    "        sample_size : int\n",
    "            The number of samples to generate.\n",
    "        num_refs : int\n",
    "            Number of reference samples for PQM.\n",
    "        range : (Int, Int, Int)\n",
    "            The linspace parameters used to\n",
    "            generate true samples used in PQMass.\n",
    "        \"\"\"\n",
    "        self.function = function\n",
    "        self.sample_size = sample_size\n",
    "        self.num_refs = num_refs\n",
    "        self.linspace = (-1, 1, 1000) if linspace is None else linspace\n",
    "\n",
    "        # Generate true samples to use in PQMass\n",
    "        self.true_samples = self.function(np.linspace(self.linspace[0], self.linspace[1], self.linspace[2]).reshape(-1, 1))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The initial state (randomly generated samples).\n",
    "        \"\"\"\n",
    "        self.state = np.random.uniform(self.linspace[0], self.linspace[1], (self.sample_size, 1))\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action and return the next state, reward, and done flag.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : np.ndarray\n",
    "            The action to take (new set of points).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The next state, reward, and done flag.\n",
    "        \"\"\"\n",
    "        # Concatenate the actions into a single array\n",
    "        self.state = action  # Update the state\n",
    "        reward = get_pqm_chi2(self.function(self.state), self.true_samples, self.num_refs)\n",
    "\n",
    "        # Optimal reward is <= num_refs-1 (chi2 degrees of freedom - 1)\n",
    "        # Set done flag if optimal reward is reached\n",
    "        done = reward <= self.num_refs - 1\n",
    "\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def get_pqm(self):\n",
    "        return get_pqm_chi2(self.state, self.true_samples, self.num_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdQnbsXE_Wdl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "YOFSPzWv_DTK"
   },
   "outputs": [],
   "source": [
    "# Function to compute returns for the critic\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute the discounted returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    next_value : torch.Tensor\n",
    "        The value of the next state.\n",
    "    rewards : list\n",
    "        List of rewards.\n",
    "    masks : list\n",
    "        List of masks indicating episode end.\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of discounted returns.\n",
    "    \"\"\"\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "  # Function to train the actor-critic model\n",
    "def training_loop(actor, critic, env, n_iters, sample_size, lr=0.0001, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Train the actor-critic model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actor : Actor\n",
    "        The actor network.\n",
    "    critic : Critic\n",
    "        The critic network.\n",
    "    env : CustomEnv\n",
    "        The custom environment.\n",
    "    n_iters : int\n",
    "        Number of training iterations.\n",
    "    sample_size : int\n",
    "        Size of the sample set.\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    max_steps : int\n",
    "        Maximum number of steps per episode.\n",
    "    \"\"\"\n",
    "    optimizerA = optim.Adam(actor.parameters(), lr)\n",
    "    optimizerC = optim.Adam(critic.parameters(), lr)\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "\n",
    "            # Get distribution of the actions\n",
    "            mean, variance = actor(state)\n",
    "            dist = Normal(mean, variance.sqrt())\n",
    "\n",
    "            # Sample the actions\n",
    "            actions = dist.sample()\n",
    "\n",
    "            # Get expected best reward\n",
    "            value = critic(state)\n",
    "\n",
    "            next_state, reward, done = env.step(actions.cpu().numpy())\n",
    "\n",
    "            print(f\"Step {i} | Reward = {reward}\")\n",
    "\n",
    "            log_prob = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "            entropy += dist.entropy().sum(dim=-1).mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            # This block executes if the loop completes without encountering a break\n",
    "            print(\"Episode terminated without reaching the done condition.\")\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        # Update models\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save(actor, 'model/actor.pkl')\n",
    "    torch.save(critic, 'model/critic.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpaCPn5fCTNQ"
   },
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "w8Bh0HzVh1zR"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define a simple function to sample from\n",
    "    def simple_function(vars, doprint=False):\n",
    "        mu, sigma = 0, 0.1\n",
    "        res = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (vars - mu)**2 / (2 * sigma**2))\n",
    "        if doprint: print(type(res), res)\n",
    "        return res\n",
    "    def simple_function2(vars, doprint=False):\n",
    "        res = np.sin(5 * vars) + 0.5 * vars\n",
    "        if doprint: print(type(res), res)\n",
    "        return res\n",
    "    \n",
    "    sample_size = 100\n",
    "    lr = 0.0001\n",
    "    env = CustomEnv(function=simple_function, sample_size=sample_size, num_refs=100, linspace=(-5, 5, 10000))\n",
    "    state_size = 1\n",
    "    do_load = False\n",
    "    \n",
    "    # Load or create actor and critic models\n",
    "    if os.path.exists('model/actor.pkl') and do_load:\n",
    "        actor = torch.load('model/actor.pkl')\n",
    "        print('Actor Model loaded')\n",
    "    else:\n",
    "        actor = Actor(state_size).to(device)\n",
    "    if os.path.exists('model/critic.pkl') and do_load:\n",
    "        critic = torch.load('model/critic.pkl')\n",
    "        print('Critic Model loaded')\n",
    "    else:\n",
    "        critic = Critic(state_size).to(device)\n",
    "\n",
    "    training_loop(actor, critic, env, n_iters=1, sample_size=sample_size, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "DJCh_dLn7JVC",
    "outputId": "98752f67-9ebf-4fb2-af03-3672bb658e11"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The internally computed table of expected frequencies has a zero element at (0, 7).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[139], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     critic \u001b[38;5;241m=\u001b[39m Critic(state_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m training_loop(actor, critic, env, n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sample_size\u001b[38;5;241m=\u001b[39msample_size, lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[114], line 74\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(actor, critic, env, n_iters, sample_size, lr, max_steps)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Get expected best reward\u001b[39;00m\n\u001b[1;32m     72\u001b[0m value \u001b[38;5;241m=\u001b[39m critic(state)\n\u001b[0;32m---> 74\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(actions)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[138], line 56\u001b[0m, in \u001b[0;36mCustomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Concatenate the actions into a single array\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m action  \u001b[38;5;66;03m# Update the state\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m reward \u001b[38;5;241m=\u001b[39m get_pqm_chi2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrue_samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_refs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Optimal reward is <= num_refs-1 (chi2 degrees of freedom - 1)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Set done flag if optimal reward is reached\u001b[39;00m\n\u001b[1;32m     60\u001b[0m done \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_refs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[41], line 41\u001b[0m, in \u001b[0;36mget_pqm_chi2\u001b[0;34m(x_samples, y_samples, num_refs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     counts_y[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Compute chi2 statistic using contingency table\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m chi2_stat, _, _, _ \u001b[38;5;241m=\u001b[39m chi2_contingency(np\u001b[38;5;241m.\u001b[39marray([counts_x, counts_y]))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chi2_stat\n",
      "File \u001b[0;32m~/.config/miniconda3/envs/stt3795/lib/python3.11/site-packages/scipy/stats/contingency.py:292\u001b[0m, in \u001b[0;36mchi2_contingency\u001b[0;34m(observed, correction, lambda_)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(expected \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Include one of the positions where expected is zero in\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# the exception message.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     zeropos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mnonzero(expected \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe internally computed table of expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequencies has a zero element at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (zeropos,))\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# The degrees of freedom\u001b[39;00m\n\u001b[1;32m    296\u001b[0m dof \u001b[38;5;241m=\u001b[39m expected\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;28msum\u001b[39m(expected\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m+\u001b[39m expected\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: The internally computed table of expected frequencies has a zero element at (0, 7)."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRmwXoClagJd"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(actor, env):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained actor model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actor : Actor\n",
    "        The trained actor model.\n",
    "    env : CustomEnv\n",
    "        The custom environment.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "\n",
    "    state_tensor = torch.FloatTensor(state).to(device)\n",
    "\n",
    "    # Get distribution of the actions from the actor\n",
    "    mean, variance = actor(state_tensor)\n",
    "    dist = Normal(mean, variance.sqrt())\n",
    "\n",
    "    # Sample an action from the distribution\n",
    "    action = dist.sample()\n",
    "    sampled_points = action.cpu().numpy()\n",
    "\n",
    "    # Take a step in the environment\n",
    "    env.step(sampled_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  [ 1.0192165  -0.7046166   0.16075183 -2.1100342   1.1247537  -0.56437665\n",
      " -2.0511155  -0.16812965  2.226415    1.0813334  -0.17472167 -1.4481682\n",
      " -0.2944311   0.28989506  1.4898683   0.7008458  -0.52827287 -1.0736123\n",
      " -0.54956603  1.0925659  -1.3257561   0.3261135  -0.39728162 -0.1515728\n",
      "  0.5025027  -1.0999614  -0.03903792  0.7862698   0.95406634  0.63724816\n",
      " -1.0876268   0.98903584  0.5373073  -0.54583764  1.2409909  -0.7909513\n",
      "  0.06562695 -0.17640752 -0.21591708  1.5101597  -0.08222449 -0.3518117\n",
      "  0.66756463  0.14549813  1.044765    0.737229   -0.9649158  -1.6232824\n",
      "  0.01011164  0.20067102]\n",
      "PQMass:  477.2452820512822\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Load the trained models\n",
    "actor = torch.load('model/actor.pkl')\n",
    "\n",
    "# Define a simple function to sample from\n",
    "def simple_function(xs):\n",
    "    res = []\n",
    "\n",
    "    for x in xs:\n",
    "        res.append(norm.pdf(x))\n",
    "    return np.array(res)\n",
    "\n",
    "sample_size = 50\n",
    "env = CustomEnv(function=simple_function, sample_size=sample_size, num_refs=50, linspace=(-1, 1, 1000))\n",
    "\n",
    "inference(actor, env)\n",
    "sampled_points = env.state.reshape(1, -1)[0]\n",
    "pqm = env.get_pqm()\n",
    "print(\"Samples: \", sampled_points)\n",
    "print(\"PQMass: \", pqm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stt3795",
   "language": "python",
   "name": "stt3795"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
