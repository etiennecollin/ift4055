@book{suttonReinforcementLearningIntroduction2014,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Sutton, Richard S and Barto, Andrew G},
  date = {2014/2015},
  edition = {Second edition, in progress},
  publisher = {The MIT Pess},
  location = {London, England},
  langid = {english},
  pagetotal = {338},
  file = {/Users/etiennecollin/Documents/Zotero/storage/G2VCI957/Sutton and Barto - Reinforcement Learning An Introduction.pdf}
}

@online{lemosPQMassProbabilisticAssessment2024,
  title = {{{PQMass}}: {{Probabilistic Assessment}} of the {{Quality}} of {{Generative Models}} Using {{Probability Mass Estimation}}},
  shorttitle = {{{PQMass}}},
  author = {Lemos, Pablo and Sharief, Sammy and Malkin, Nikolay and Perreault-Levasseur, Laurence and Hezaveh, Yashar},
  date = {2024-02-06},
  eprint = {2402.04355},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2402.04355},
  url = {http://arxiv.org/abs/2402.04355},
  urldate = {2024-05-13},
  abstract = {We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) across various sub-regions within the data space.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/etiennecollin/Documents/Zotero/storage/UMRERMAJ/Lemos et al. - 2024 - PQMass Probabilistic Assessment of the Quality of.pdf;/Users/etiennecollin/Documents/Zotero/storage/ILTY9PTZ/2402.html}
}

@online{hernandez-garciaMultiFidelityActiveLearning2023,
  title = {Multi-{{Fidelity Active Learning}} with {{GFlowNets}}},
  author = {Hernandez-Garcia, Alex and Saxena, Nikita and Jain, Moksh and Liu, Cheng-Hao and Bengio, Yoshua},
  date = {2023-06-20},
  eprint = {2306.11715},
  eprinttype = {arXiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2306.11715},
  url = {http://arxiv.org/abs/2306.11715},
  urldate = {2024-05-13},
  abstract = {In the last decades, the capacity to generate large amounts of data in science and engineering applications has been growing steadily. Meanwhile, the progress in machine learning has turned it into a suitable tool to process and utilise the available data. Nonetheless, many relevant scientific and engineering problems present challenges where current machine learning methods cannot yet efficiently leverage the available data and resources. For example, in scientific discovery, we are often faced with the problem of exploring very large, high-dimensional spaces, where querying a high fidelity, black-box objective function is very expensive. Progress in machine learning methods that can efficiently tackle such problems would help accelerate currently crucial areas such as drug and materials discovery. In this paper, we propose the use of GFlowNets for multi-fidelity active learning, where multiple approximations of the black-box function are available at lower fidelity and cost. GFlowNets are recently proposed methods for amortised probabilistic inference that have proven efficient for exploring large, high-dimensional spaces and can hence be practical in the multi-fidelity setting too. Here, we describe our algorithm for multi-fidelity active learning with GFlowNets and evaluate its performance in both well-studied synthetic tasks and practically relevant applications of molecular discovery. Our results show that multi-fidelity active learning with GFlowNets can efficiently leverage the availability of multiple oracles with different costs and fidelities to accelerate scientific discovery and engineering design.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/etiennecollin/Documents/Zotero/storage/88WGAG77/Hernandez-Garcia et al. - 2023 - Multi-Fidelity Active Learning with GFlowNets.pdf;/Users/etiennecollin/Documents/Zotero/storage/3XITDZ2X/2306.html}
}

@article{kamathIntelligentSamplingSurrogate2022,
  title = {Intelligent Sampling for Surrogate Modeling, Hyperparameter Optimization, and Data Analysis},
  author = {Kamath, Chandrika},
  date = {2022-09-15},
  journaltitle = {Machine Learning with Applications},
  shortjournal = {Machine Learning with Applications},
  volume = {9},
  pages = {100373},
  issn = {2666-8270},
  doi = {10.1016/j.mlwa.2022.100373},
  url = {https://www.sciencedirect.com/science/article/pii/S2666827022000640},
  urldate = {2024-05-13},
  abstract = {Sampling techniques are used in many fields, including design of experiments, image processing, and graphics. The techniques in each field are designed to meet the constraints specific to that field such as uniform coverage of the range of each dimension or random samples that are at least a certain distance apart from each other. When an application imposes new constraints, for example, by requiring samples in a non-rectangular domain or the addition of new samples to an existing set, a common solution is to modify the algorithm currently in use, often with less than satisfactory results. As an alternative, we propose the concept of intelligent sampling, where we devise solutions specifically tailored to meet our sampling needs, either by improving existing algorithms or by modifying suitable algorithms from other fields. Surprisingly, both qualitative and quantitative comparisons indicate that some relatively simple algorithms can be easily modified to meet the many sampling requirements of surrogate modeling, hyperparameter optimization, and data analysis; these algorithms outperform their more sophisticated counterparts currently in use, resulting in better use of time and computer resources.},
  langid = {english},
  keywords = {Data analysis,Hyperparameter optimization,Sampling,Space filling,Surrogate modeling},
  file = {/Users/etiennecollin/Documents/Zotero/storage/NGZQX8VJ/Kamath - 2022 - Intelligent sampling for surrogate modeling, hyper.pdf;/Users/etiennecollin/Documents/Zotero/storage/WBMUEDSF/S2666827022000640.html}
}

@online{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  date = {2019-07-05},
  eprint = {1509.02971},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.02971},
  url = {http://arxiv.org/abs/1509.02971},
  urldate = {2024-05-20},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/7EX4QM2K/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf;/Users/etiennecollin/Documents/Zotero/storage/6D8ISTDL/1509.html}
}

@online{malkinGFlowNetsVariationalInference2023,
  title = {{{GFlowNets}} and Variational Inference},
  author = {Malkin, Nikolay and Lahlou, Salem and Deleu, Tristan and Ji, Xu and Hu, Edward and Everett, Katie and Zhang, Dinghuai and Bengio, Yoshua},
  date = {2023-03-02},
  eprint = {2210.00580},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2210.00580},
  url = {http://arxiv.org/abs/2210.00580},
  urldate = {2024-06-03},
  abstract = {This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/ICGBC9YB/Malkin et al. - 2023 - GFlowNets and variational inference.pdf;/Users/etiennecollin/Documents/Zotero/storage/GWAVR99A/2210.html}
}

@online{maravalEndtoEndMetaBayesianOptimisation2023,
  title = {End-to-{{End Meta-Bayesian Optimisation}} with {{Transformer Neural Processes}}},
  author = {Maraval, Alexandre and Zimmer, Matthieu and Grosnit, Antoine and Ammar, Haitham Bou},
  date = {2023-12-22},
  eprint = {2305.15930},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.15930},
  url = {http://arxiv.org/abs/2305.15930},
  urldate = {2024-06-11},
  abstract = {Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL objective with an auxiliary task that guides part of the architecture to learn a valid probabilistic model as an inductive bias. We demonstrate that our method achieves state-of-the-art regret results against various baselines in experiments on standard hyperparameter optimisation tasks and also outperforms others in the real-world problems of mixed-integer programming tuning, antibody design, and logic synthesis for electronic design automation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/TVZ37U9H/Maraval et al. - 2023 - End-to-End Meta-Bayesian Optimisation with Transfo.pdf;/Users/etiennecollin/Documents/Zotero/storage/HSIFVIWI/2305.html}
}

@online{volppMetaLearningAcquisitionFunctions2020,
  title = {Meta-{{Learning Acquisition Functions}} for {{Transfer Learning}} in {{Bayesian Optimization}}},
  author = {Volpp, Michael and Fröhlich, Lukas P. and Fischer, Kirsten and Doerr, Andreas and Falkner, Stefan and Hutter, Frank and Daniel, Christian},
  date = {2020-02-14},
  eprint = {1904.02642},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1904.02642},
  url = {http://arxiv.org/abs/1904.02642},
  urldate = {2024-06-14},
  abstract = {Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/HS5PTDNL/Volpp et al. - 2020 - Meta-Learning Acquisition Functions for Transfer L.pdf;/Users/etiennecollin/Documents/Zotero/storage/6TTVX3TV/1904.html}
}

@online{wistubaFewShotBayesianOptimization2021,
  title = {Few-{{Shot Bayesian Optimization}} with {{Deep Kernel Surrogates}}},
  author = {Wistuba, Martin and Grabocka, Josif},
  date = {2021-01-19},
  eprint = {2101.07667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2101.07667},
  url = {http://arxiv.org/abs/2101.07667},
  urldate = {2024-06-14},
  abstract = {Hyperparameter optimization (HPO) is a central pillar in the automation of machine learning solutions and is mainly performed via Bayesian optimization, where a parametric surrogate is learned to approximate the black box response function (e.g. validation error). Unfortunately, evaluating the response function is computationally intensive. As a remedy, earlier work emphasizes the need for transfer learning surrogates which learn to optimize hyperparameters for an algorithm from other tasks. In contrast to previous work, we propose to rethink HPO as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surrogate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. As a result, the novel few-shot optimization of our deep kernel surrogate leads to new state-of-the-art results at HPO compared to several recent methods on diverse metadata sets.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/A4QSNY4H/Wistuba and Grabocka - 2021 - Few-Shot Bayesian Optimization with Deep Kernel Su.pdf;/Users/etiennecollin/Documents/Zotero/storage/Z89WHUAV/2101.html}
}

@online{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  date = {2020-11-07},
  eprint = {2004.05439},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2004.05439},
  url = {http://arxiv.org/abs/2004.05439},
  urldate = {2024-07-10},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/QCVFVT7N/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf;/Users/etiennecollin/Documents/Zotero/storage/EXHDAXFB/2004.html}
}

@online{bengioGFlowNetTutorial2022,
  title = {The {{GFlowNet Tutorial}}},
  author = {Bengio, Yoshua and Malkin, Kolya and Jain, Moksh},
  date = {2022},
  url = {https://yoshuabengio.org/gflownet-tutorial},
  urldate = {2024-05-20},
  abstract = {A GFlowNet is a trained stochastic policy or generative model, trained such that it samples objects \$x\$ through a sequence of constructive steps, with probability proportional to a reward function \$R(x)\$, where \$R\$ is a non-negative integrable function. This makes a GFlowNet able to sample a diversity of solutions \$x\$ that have a high value of \$R(x)\$.},
  langid = {english},
  organization = {Notion},
  file = {/Users/etiennecollin/Documents/Zotero/storage/2IY8RJV2/The-GFlowNet-Tutorial-95434ef0e2d94c24aab90e69b30be9b3.html}
}

@online{erikssonHighDimensionalBayesianOptimization2021,
  title = {High-{{Dimensional Bayesian Optimization}} with {{Sparse Axis-Aligned Subspaces}}},
  author = {Eriksson, David and Jankowiak, Martin},
  date = {2021-06-10},
  eprint = {2103.00349},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2103.00349},
  url = {http://arxiv.org/abs/2103.00349},
  urldate = {2024-07-09},
  abstract = {Bayesian optimization (BO) is a powerful paradigm for efficient optimization of black-box objective functions. High-dimensional BO presents a particular challenge, in part because the curse of dimensionality makes it difficult to define -- as well as do inference over -- a suitable class of surrogate models. We argue that Gaussian process surrogate models defined on sparse axis-aligned subspaces offer an attractive compromise between flexibility and parsimony. We demonstrate that our approach, which relies on Hamiltonian Monte Carlo for inference, can rapidly identify sparse subspaces relevant to modeling the unknown objective function, enabling sample-efficient high-dimensional BO. In an extensive suite of experiments comparing to existing methods for high-dimensional BO we demonstrate that our algorithm, Sparse Axis-Aligned Subspace BO (SAASBO), achieves excellent performance on several synthetic and real-world problems without the need to set problem-specific hyperparameters.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/KIDKY5SP/Eriksson and Jankowiak - 2021 - High-Dimensional Bayesian Optimization with Sparse.pdf;/Users/etiennecollin/Documents/Zotero/storage/UWU4RZDF/2103.html}
}

@online{balandatBoTorchFrameworkEfficient2020,
  title = {{{BoTorch}}: {{A Framework}} for {{Efficient Monte-Carlo Bayesian Optimization}}},
  shorttitle = {{{BoTorch}}},
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  date = {2020-12-08},
  eprint = {1910.06403},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1910.06403},
  url = {http://arxiv.org/abs/1910.06403},
  urldate = {2024-08-05},
  abstract = {Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel "one-shot" formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.},
  pubstate = {prepublished},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/GH7AVF3T/Balandat et al. - 2020 - BoTorch A Framework for Efficient Monte-Carlo Bay.pdf;/Users/etiennecollin/Documents/Zotero/storage/4UIEVAG4/1910.html}
}

@audio{edwardhuAreGFlowNetsFuture2024,
  title = {Are {{GFlowNets}} the Future of {{AI}}?},
  namea = {{Edward Hu}},
  nameatype = {collaborator},
  date = {2024-03-13},
  url = {https://www.youtube.com/watch?v=o0Ju9NQa5Ko},
  urldate = {2024-08-06},
  abstract = {Should you care about GFlowNets? What are they anyway? Learn about how GFlowNets are aiding drug discovery and reasoning in large language models! *Like, subscribe, and share if you find this video valuable!* Tutorial: https://milayb.notion.site/The-GFlowN... 0:00 - Why care about GFlowNets? 0:54 - The problems GFlowNets solve 1:39 - A concrete example: drug discovery 3:53 - What GFlowNet really is 4:46 - Applications: GFlowNet-EM 5:58 - Applications: Better LLM reasoning 6:55 - Conclusion Papers mentioned: - GFlowNet for drug discovery (first GFlowNet paper) https://arxiv.org/abs/2106.04399 - Jointly training a GFlowNet and an energy-based model https://arxiv.org/abs/2202.01361 - GFlowNet-EM https://arxiv.org/abs/2302.06576 - GFlowNet for better reasoning in LLMs https://arxiv.org/pdf/2310.04363.pdf Follow me on Twitter: ~~/~edwardjhu~~ 🙏 This video would not be possible without my wonderful labmates at Mila and, of course, Yoshua.},
  langid = {english}
}

@online{lahlouTorchgfnPyTorchGFlowNet2023,
  title = {Torchgfn: {{A PyTorch GFlowNet}} Library},
  shorttitle = {Torchgfn},
  author = {Lahlou, Salem and Viviano, Joseph D. and Schmidt, Victor and Bengio, Yoshua},
  date = {2023-08-29},
  eprint = {2305.14594},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.14594},
  url = {http://arxiv.org/abs/2305.14594},
  urldate = {2024-08-07},
  abstract = {The growing popularity of generative flow networks (GFlowNets or GFNs) from a range of researchers with diverse backgrounds and areas of expertise necessitates a library which facilitates the testing of new features such as training losses that can be easily compared to standard benchmark implementations, or on a set of common environments. torchgfn is a PyTorch library that aims to address this need. It provides users with a simple API for environments and useful abstractions for samplers and losses. Multiple examples are provided, replicating and unifying published results. The code is available in https://github.com/saleml/torchgfn.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/N9VRVJMC/Lahlou et al. - 2023 - torchgfn A PyTorch GFlowNet library.pdf;/Users/etiennecollin/Documents/Zotero/storage/HR6RA5MW/2305.html}
}

@online{malkinTrajectoryBalanceImproved2023,
  title = {Trajectory Balance: {{Improved}} Credit Assignment in {{GFlowNets}}},
  shorttitle = {Trajectory Balance},
  author = {Malkin, Nikolay and Jain, Moksh and Bengio, Emmanuel and Sun, Chen and Bengio, Yoshua},
  date = {2023-10-04},
  eprint = {2201.13259},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2201.13259},
  url = {http://arxiv.org/abs/2201.13259},
  urldate = {2024-08-11},
  abstract = {Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets, flow matching and detailed balance, which are analogous to temporal difference learning, to be prone to inefficient credit propagation across long action sequences. We thus propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/57GYSTGH/Malkin et al. - 2023 - Trajectory balance Improved credit assignment in .pdf;/Users/etiennecollin/Documents/Zotero/storage/LS397MPC/2201.html}
}

@online{madanLearningGFlowNetsPartial2023,
  title = {Learning {{GFlowNets}} from Partial Episodes for Improved Convergence and Stability},
  author = {Madan, Kanika and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Emmanuel and Jain, Moksh and Nica, Andrei and Bosc, Tom and Bengio, Yoshua and Malkin, Nikolay},
  date = {2023-06-03},
  eprint = {2209.12782},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2209.12782},
  url = {http://arxiv.org/abs/2209.12782},
  urldate = {2024-08-11},
  abstract = {Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD(\$\textbackslash lambda\$) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB(\$\textbackslash lambda\$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB(\$\textbackslash lambda\$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/QEN6PNBN/Madan et al. - 2023 - Learning GFlowNets from partial episodes for impro.pdf;/Users/etiennecollin/Documents/Zotero/storage/PUR98SFN/2209.html}
}

@online{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  date = {2019-05-26},
  eprint = {1810.00825},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.00825},
  url = {http://arxiv.org/abs/1810.00825},
  urldate = {2024-08-12},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/etiennecollin/Documents/Zotero/storage/4MGLGUKA/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf;/Users/etiennecollin/Documents/Zotero/storage/LHDVXUFG/1810.html}
}

@software{ciela-institutePQMassProbabilisticAssessment2024,
  title = {{{PQMass}}: {{Probabilistic Assessment}} of the {{Quality}} of {{Generative Models}} Using {{Probability Mass Estimation}}},
  shorttitle = {{{PQMass}}},
  author = {{Ciela-Institute}},
  date = {2024-08-19T18:28:48Z},
  origdate = {2024-05-29T20:44:27Z},
  url = {https://github.com/Ciela-Institute/PQM},
  urldate = {2024-08-24},
  abstract = {Implemenation of PQMass from Lemos et al. 2024},
  organization = {Ciela Institute},
  version = {0.5.1}
}
