#[derive(Clone, Default, Eq, PartialEq)]
pub enum Languages {
    #[default]
    English,
    French,
}

#[derive(Clone, Default)]
pub struct Locale<'a> {
    pub progress_1_title: &'a str,
    pub progress_1_description: &'a str,
    pub progress_2_title: &'a str,
    pub progress_2_description: &'a str,
    pub progress_3_title: &'a str,
    pub progress_3_description: &'a str,
    pub progress_4_title: &'a str,
    pub progress_4_description: &'a str,
    pub progress_5_title: &'a str,
    pub progress_5_description: &'a str,
    pub progress_title: &'a str,
    pub lang_button_text: &'a str,
    pub sidebar_title: &'a str,
    pub sidebar_description: &'a str,
    pub planning_title: &'a str,
    pub planning: &'a str,
    pub description_title: &'a str,
    pub description: &'a str,
}

pub const LOCALE_EN: Locale<'static> = Locale {
    progress_1_title: "Creation of website, getting familiar with problem and reading",
    progress_1_description: "
        To optimize the sampling process in the reinforcement learning network, the reward function must to quantify how well the points provided by the network represent the function from which they are sampled. Put simply, it should measure the disparity between the original function and the sampled points generated by the model.

        <br/>
        <a href=\"https://arxiv.org/abs/2402.04355\">PQMass</a> serves as a viable candidate for this purpose, as it quantifies the probability that two sets of samples originate from the same distribution. Therefore, by uniformly sampling points from simple functions and evaluating them against the model's output using the aforementioned metric, the model can learn to sample efficiently, ensuring that the points accurately represent the function with minimal sampling.

        <br/>
        A question remains: how to model the state/the actions of the model and the continuous action space that shouldn't be discretized? How should the model update how it samples points given its reward/score?
    ",
    progress_2_title: "Continuous RL, implementation, RL workshop and move to GFlowNets",
    progress_2_description: "
        To solve the issue of discretization, the actor-critic algorithm was researched, allowing for continuous action spaces by creating a model which returns the probability distribution of the actions from which it is possible to sample an action. A basic implementation of the actor-critic algorith was created with PQMass as the reward function. I also attended a MILA workshop on RL 

        <br/>
        After discussing with my supervisor, it was decided that classic RL (DQN, actor-critic, etc) was not the best approach for my problem. Indeed, these methods have a harder time learning the various modes that can be present in the function space. GFlowNets offer an architecture which explores that function space better. Research was done to understand how GFlowNets work and how they can be used to sample points in a function space. A simple implementation of GFlowNets, sampling basic functions, was created following tutorials from the <a href=\"https://github.com/GFNOrg/torchgfn/\">torchgfn python package</a>. GFlowNets seem promising!
    ",
    progress_3_title: "GFlowNets, Bayesian Optimisation, Gaussian Processes and Acquisition Function",
    progress_3_description: "
        After working on the GFlowNet implementation, it is clear that this architecture is promising for the problem at hand. That being said, the GFlowNet used had trouble finding all of the modes in a sparse distribution. This is probably caused by the usage of an \"on-policy\" model, meaning that the model fould sample its next points from the learned policy. This should not be the case in the final project, because the acquisition model will be used to propose the points.

        <br/>
        Up until now, it is therefore clear that a GFlowNet will be used as a surrogate model to emulate/approximate the complex distribution/function. Moreover after discussing with my supervisors, I will be exploring how gaussian processes and acquisition functions (AF) are used in the context of bayesian optimisation (BO). At first glance, it seems that this project is closely related to BO. In fact, the main difference is that BO only cares about accurately finding the maximal/minimal value of a function (because it is an optimisation problem), whereas this project aims to accurately model the entire function and sample from it.

        <br/>
        The next step is to research on papers discussing the use of neural networks as AFs in BO in order to learn how they work and how to adapt them to my problem that is not an optimisation problem.
    ",
    progress_4_title: "Few Shot Learning and Meta Learning",
    progress_4_description: "
        After reading on BO, I stumbled on papers researching the use of meta learning and few shot learning in the context of BO. This is interesting because it could potentially help in the context of my project as my acquisition network should be able to learn from a class of problems and be able to make decisions on new problems it has never seen before (assuming it is also in that class of problems).

        <br/>
        Reading these papers and their code was interesting, but I was lacking the theoretical background to understand everything. I therefore followed online courses on meta learning and few shot learning before revisiting these papers.

        <br/>
        I also met a PhD student with my supervisor at MILA who used to work on BO. This meeting was helpful and it was concluded that I should try to find a way to introduce some notion of uncertainty/variance in the surrogate network. That is, instead of outputting a point y for a given x, the surrogate network should output the mean and variance of y for a given x. This will be used to inform the acquisition model where it should sample next.
    ",
    progress_5_title: "todo",
    progress_5_description: "
        Description
    ",
    progress_title: "Progress",
    lang_button_text: "Toggle Language",
    sidebar_title: "IFT4055 - Honors Project",
    sidebar_description: "Built with love using Rust ü¶Ä and NeoVim üñ•Ô∏è",
    planning_title: "Project Schedule",
    planning: "
        <p class=\"text-lighttext-800 dark:text-darktext-200\">
            This entire project should span the 4 months of summer 2024. Here is a general outlook on the schedule.
        </p>
        <ul>
            <li>May: Planning, reading and initial research</li>
            <li>June: Implementation of models</li>
            <li>July: Debugging and generation of results</li>
            <li>August: Writing for paper</li>
        </ul>",
    description_title: "Efficient Function Evaluation Using Reinforcement Learning",
    description: "
        <p>
            In scientific endeavors, the evaluation of complex functions, often spanning multiple dimensions, presents a significant computational challenge. These functions can be costly and time-consuming to evaluate accurately. The project proposes the development of a novel approach to address this challenge by leveraging neural networks and reinforcement learning techniques to emulate the evaluation of arbitrarily complex functions efficiently.
        </p>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Objectives</h3>
        <p>
            The primary objective of the research project is to train a neural network capable of emulating the evaluation of complex functions in a fraction of the time it would take to evaluate the true function directly. Specifically, the project aims to:
        </p>
        <ul>
            <li>
                Develop a neural network architecture capable of accurately emulating complex functions.
            </li>
            <li>
                Train the neural network efficiently using reinforcement learning techniques to sample points in the function space effectively.
            </li>
            <li>
                Minimize the computational cost associated with evaluating complex functions by optimizing the sampling strategy.
            </li>
        </ul>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Methodology</h3>
        <p>The research will employ a two-step approach:</p>
        <ol>
            <li>
                <strong>Reinforcement Learning for Efficient Sampling: </strong>
                A reinforcement learning model will be trained to sample points in the function space effectively. The model's objective will be to learn how to select points that contribute the most to accurately representing the function while minimizing the number of evaluations required. The reinforcement learning model will use a non-differentiable function as a reward signal, representing the discrepancy between the true function and its emulation. This function will guide the model to sample points efficiently.
            </li>
            <li>
                <strong>Neural Network Emulation: </strong>
                A neural network will be trained using the sampled points generated by the reinforcement learning model. This neural network will learn to approximate the complex function using the sampled data points. By emulating the function, the neural network will enable rapid evaluation of the function at any given input, significantly reducing computational overhead compared to direct evaluation.
            </li>
        </ol>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Expected Outcomes</h3>
        <ul>
            <li>
                Development of a novel approach for efficiently evaluating complex functions using neural networks and reinforcement learning.
            </li>
            <li>
                A trained neural network capable of accurately emulating complex functions, significantly reducing evaluation time.
            </li>
            <li>
                Insights into efficient sampling strategies for complex function spaces, applicable beyond the scope of this research.
            </li>
        </ul>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">
            Significance and Potential Applications
        </h3>
        <p>
            Efficient function evaluation is critical in various scientific disciplines, including physics, engineering, and machine learning. The proposed approach has the potential to revolutionize computational methods by enabling faster and more efficient evaluation of complex functions. Applications include optimization problems, simulation-based analysis, and model training in various domains.
        </p>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Timeline and Resources</h3>
        <p>
            The research will be conducted over a specified timeline, leveraging computational resources for model training and validation. Collaboration with experts in machine learning, optimization, and domain-specific areas will enrich the research process and ensure its success.
        </p>

        <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Conclusion</h3>
        <p>
            The project proposes a novel approach to address the computational challenges associated with evaluating complex functions efficiently. By combining neural networks and reinforcement learning techniques, the research aims to develop a scalable solution applicable to a wide range of scientific and engineering domains. This innovative approach has the potential to accelerate progress in computational science and enable breakthroughs in complex problem-solving.
        </p>
    ",
};

pub const LOCALE_FR: Locale<'static> = Locale {
    progress_1_title: "Cr√©ation du site web, familiarisation avec le probl√®me et lecture",
    progress_1_description: "
        Pour optimiser le processus d'√©chantillonnage dans le r√©seau d'apprentissage par renforcement, la fonction de r√©compense doit quantifier la qualit√© des points fournis en s'assurant qu'ils repr√©sentent la fonction √† partir de laquelle ils sont √©chantillonn√©s. En d'autres termes, elle doit mesurer la disparit√© entre la fonction originale et les points √©chantillonn√©s g√©n√©r√©s par le mod√®le.

        <br/>
        <a href=\"https://arxiv.org/abs/2402.04355\">PQMass</a> constitue un candidat viable √† cette fin, car cette technique quantifie la probabilit√© que deux ensembles d'√©chantillons proviennent de la m√™me distribution. Par cons√©quent, en √©chantillonnant uniform√©ment des points √† partir de fonctions simples et en les √©valuant par rapport √† la sortie du mod√®le en utilisant la m√©trique mentionn√©e pr√©c√©demment, le mod√®le peut apprendre √† √©chantillonner efficacement, garantissant que les points repr√©sentent avec pr√©cision la fonction avec un √©chantillonnage minimal.

        <br/>
        Une question demeure: comment mod√©liser l'√©tat/les actions du mod√®le et comment g√©rer l'espace d'actions continu qui ne doit pas √™tre discr√©tis√©? Comment le mod√®le devrait-il mettre √† jour sa mani√®re d'√©chantillonner des points compte tenu de sa r√©compense/son score?
    ",
    progress_2_title: "RL continu, impl√©mentation, atelier RL et transition vers les GFlowNets",
    progress_2_description: "
        Pour r√©soudre le probl√®me de discr√©tisation, l'algorithme acteur-critique a √©t√© √©tudi√©, permettant des espaces d'action continus en cr√©ant un mod√®le qui retourne la distribution de probabilit√© des actions √† partir de laquelle il est possible d'√©chantillonner une action. Une impl√©mentation de base de l'algorithme acteur-critique a √©t√© cr√©√©e avec PQMass comme fonction de r√©compense. J'ai √©galement assist√© √† un atelier de MILA sur l'apprentissage par renforcement.

        <br/>
        Apr√®s avoir discut√© avec mon superviseur, il a √©t√© d√©cid√© que l'apprentissage par renforcement classique (DQN, acteur-critique, etc.) n'√©tait pas la meilleure approche pour mon probl√®me. En effet, ces m√©thodes ont plus de difficult√© √† apprendre les diff√©rentes modalit√©s pouvant √™tre pr√©sentes dans l'espace fonctionnel. Les GFlowNets offrent une architecture qui explore mieux cet espace fonctionnel. Des recherches ont √©t√© effectu√©es pour comprendre comment fonctionnent les GFlowNets et comment ils peuvent √™tre utilis√©s pour √©chantillonner des points dans un espace fonctionnel. Une impl√©mentation simple de GFlowNets, √©chantillonnant des fonctions de base, a √©t√© cr√©√©e en suivant les tutoriels du <a href=\"https://github.com/GFNOrg/torchgfn/\">package python torchgfn</a>. Les GFlowNets semblent prometteurs!
    ",
    progress_3_title: "GFlowNets, optimisation bay√©sienne, processus gaussiens et fonctions d'acquisitions",
    progress_3_description: "
        Apr√®s avoir travaill√© sur l'impl√©mentation de GFlowNet, il est clair que cette architecture est prometteuse pour le probl√®me en question. Cela √©tant dit, le GFlowNet utilis√© a eu des difficult√©s √† trouver tous les modes dans une distribution √©parse. Cela est probablement d√ª √† l'utilisation d'un mod√®le \"on-policy\", ce qui signifie que le mod√®le √©chantillonnait ses prochains points √† partir de la politique apprise. Cela ne devrait pas √™tre le cas dans le projet final, car le mod√®le d'acquisition sera utilis√© pour proposer les points.

        <br/>
        Jusqu'√† pr√©sent, il est donc clair qu'un GFlowNet sera utilis√© comme mod√®le de substitution pour √©muler/approximer la distribution/fonction complexe. De plus, apr√®s avoir discut√© avec mes superviseurs, j'explorerai comment les processus gaussiens et les fonctions d'acquisition (FA) sont utilis√©s dans le contexte de l'optimisation bay√©sienne (OB). √Ä premi√®re vue, il semble que ce projet soit √©troitement li√© √† l'OB. En fait, la principale diff√©rence est que l'OB se pr√©occupe uniquement de trouver avec pr√©cision la valeur maximale/minimale d'une fonction (car c'est un probl√®me d'optimisation), alors que ce projet vise √† mod√©liser avec pr√©cision toute la fonction et √† en √©chantillonner.

        <br/>
        La prochaine √©tape consiste √† rechercher des articles discutant de l'utilisation des r√©seaux neuronaux comme FAs dans l'OB afin d'apprendre comment ils fonctionnent et comment les adapter √† mon probl√®me qui n'est pas un probl√®me d'optimisation.
    ",
    progress_4_title: "Apprentissage par quelques exemples et m√©ta-apprentissage",
    progress_4_description: "
        Apr√®s avoir lu sur l'optimisation bay√©sienne (OB), je suis tomb√© sur des articles de recherche concernant l'utilisation du m√©ta-apprentissage et de l'apprentissage par petits √©chantillons dans le contexte de l'OB. C'est int√©ressant car cela pourrait potentiellement aider dans le cadre de mon projet, √©tant donn√© que mon r√©seau d'acquisition devrait √™tre capable d'apprendre √† partir d'une classe de probl√®mes et √™tre capable de prendre des d√©cisions sur de nouveaux probl√®mes qu'il n'a jamais vus auparavant (en supposant qu'ils appartiennent √©galement √† cette classe de probl√®mes).

        <br/>
        La lecture de ces articles et de leur code √©tait int√©ressante, mais il me manquait le bagage th√©orique pour tout comprendre. J'ai donc suivi des cours en ligne sur le m√©ta-apprentissage et l'apprentissage par petits √©chantillons avant de retourner √† ces articles.

        <br/>
        J'ai √©galement rencontr√© un √©tudiant en doctorat avec mon superviseur √† MILA, qui travaillait auparavant sur l'OB. Cette rencontre a √©t√© utile et il a √©t√© conclu que je devrais essayer de trouver un moyen d'introduire une notion d'incertitude/de variance dans le r√©seau de substitution. C'est-√†-dire que, au lieu de donner un point y pour un x donn√©, le r√©seau de substitution devrait donner la moyenne et la variance de y pour un x donn√©. Cela sera utilis√© pour informer le mod√®le d'acquisition sur l'endroit o√π il doit √©chantillonner ensuite.
    ",
    progress_5_title: "todo",
    progress_5_description: "
        Description
    ",
    progress_title: "Progr√®s",
    lang_button_text: "Changer de Langue",
    sidebar_title: "IFT4055 - Projet Honor",
    sidebar_description: "Construit avec passion en utilisant Rust ü¶Ä et NeoVim üñ•Ô∏è",
    planning_title: "Calendrier du Projet",
    planning: "
        <p class=\"text-lighttext-800 dark:text-darktext-200\">
            L'ensemble de ce projet devrait s'√©tendre sur les 4 mois de l'√©t√© 2024. Voici un aper√ßu g√©n√©ral du calendrier.
        </p>
        <ul>
            <li>Mai: Planification, lecture et recherche initiale</li>
            <li>Juin: Impl√©mentation des mod√®les</li>
            <li>Juillet: D√©bogage et g√©n√©ration des r√©sultats</li>
            <li>Ao√ªt: R√©daction du document</li>
        </ul>
    ",
    description_title: "√âvaluation efficace des fonctions √† l'aide de l'apprentissage par renforcement",
    description: "
        <div class=\"text-lighttext-800 dark:text-darktext-200\">
            <p>
                En sciences, l'√©valuation de fonctions complexes, souvent s'√©tendant sur plusieurs dimensions, pr√©sente un d√©fi computationnel significatif. Ces fonctions peuvent √™tre co√ªteuses et prendre beaucoup de temps √† √©valuer avec pr√©cision. Le projet propose le d√©veloppement d'une approche novatrice pour relever ce d√©fi en tirant profit des r√©seaux neuronaux et des techniques d'apprentissage par renforcement pour √©muler efficacement l'√©valuation de fonctions arbitrairement complexes.
            </p>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Objectifs</h3>
            <p>
                L'objectif principal du projet de recherche est de former un r√©seau neuronal capable d'√©muler l'√©valuation de fonctions complexes en une fraction du temps n√©cessaire pour √©valuer directement la vraie fonction. Plus pr√©cis√©ment, le projet vise √† :
            </p>
            <ul>
                <li>
                    D√©velopper une architecture de r√©seau neuronal capable d'√©muler avec pr√©cision des fonctions complexes.
                </li>
                <li>
                    Entra√Æner le r√©seau neuronal de mani√®re efficace en utilisant des techniques d'apprentissage par renforcement pour √©chantillonner efficacement des points dans l'espace des fonctions.
                </li>
                <li>
                    Minimiser le co√ªt computationnel associ√© √† l'√©valuation de fonctions complexes en optimisant la strat√©gie d'√©chantillonnage.
                </li>
            </ul>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">M√©thodologie</h3>
            <p>La recherche utilisera une approche en deux √©tapes :</p>
            <ol>
                <li>
                    <strong>Apprentissage par Renforcement pour un √âchantillonnage Efficace : </strong>
                    Un mod√®le d'apprentissage par renforcement sera form√© pour √©chantillonner efficacement des points dans l'espace des fonctions. L'objectif du mod√®le sera d'apprendre √† s√©lectionner des points qui contribuent le plus √† repr√©senter avec pr√©cision la fonction tout en minimisant le nombre d'√©valuations n√©cessaires. Le mod√®le d'apprentissage par renforcement utilisera une fonction non diff√©rentiable comme signal de r√©compense, repr√©sentant la disparit√© entre la vraie fonction et son √©mulation. Cette fonction guidera le mod√®le pour √©chantillonner des points efficacement.
                </li>
                <li>
                    <strong>√âmulation par R√©seau Neuronal : </strong>
                    Un r√©seau neuronal sera form√© en utilisant les points √©chantillonn√©s g√©n√©r√©s par le mod√®le d'apprentissage par renforcement. Ce r√©seau neuronal apprendra √† approximer la fonction complexe en utilisant les points de donn√©es √©chantillonn√©s. En √©mulant la fonction, le r√©seau neuronal permettra une √©valuation rapide de la fonction √† n'importe quelle entr√©e donn√©e, r√©duisant ainsi consid√©rablement les frais informatiques par rapport √† une √©valuation directe.
                </li>
            </ol>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">R√©sultats Attendus</h3>
            <ul>
                <li>
                    D√©veloppement d'une approche novatrice pour √©valuer efficacement des fonctions complexes en utilisant des r√©seaux neuronaux et l'apprentissage par renforcement.
                </li>
                <li>
                    Un r√©seau neuronal form√© capable d'√©muler avec pr√©cision des fonctions complexes, r√©duisant significativement le temps d'√©valuation.
                </li>
                <li>
                    Perspectives sur les strat√©gies d'√©chantillonnage efficaces pour les espaces de fonctions complexes, applicables au-del√† du champ de cette recherche.
                </li>
            </ul>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">
                Signification et Applications Potentielles
            </h3>
            <p>
                L'√©valuation efficace des fonctions est cruciale dans diverses disciplines scientifiques, notamment la physique, l'ing√©nierie et l'apprentissage automatique. L'approche propos√©e a le potentiel de r√©volutionner les m√©thodes computationnelles en permettant une √©valuation plus rapide et plus efficace de fonctions complexes. Les applications incluent les probl√®mes d'optimisation, l'analyse bas√©e sur la simulation et la formation de mod√®les dans divers domaines.
            </p>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Calendrier et Ressources</h3>
            <p>
                La recherche sera men√©e sur une p√©riode d√©finie, en utilisant des ressources informatiques pour la formation et la validation du mod√®le. La collaboration avec des experts en apprentissage automatique, en optimisation et dans des domaines sp√©cifiques enrichira le processus de recherche et garantira son succ√®s.
            </p>

            <h3 class=\"text-lightaccent-600 dark:text-darkaccent-400\">Conclusion</h3>
            <p>
                Le projet propose une approche novatrice pour relever les d√©fis computationnels associ√©s √† l'√©valuation efficace de fonctions complexes. En combinant des r√©seaux neuronaux et des techniques d'apprentissage par renforcement, la recherche vise √† d√©velopper une solution √©volutive applicable √† un large √©ventail de domaines scientifiques et d'ing√©nierie. Cette approche innovante a le potentiel d'acc√©l√©rer les progr√®s en sciences computationnelles et de permettre des perc√©es dans la r√©solution de probl√®mes complexes.
            </p>
        </div>
    ",
};
